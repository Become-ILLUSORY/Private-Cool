import requests
import json
import re
from typing import List, Dict

class SiteFinderWithHashFields:
    def __init__(self):
        self.results: List[Dict] = []
        self.last_searched_names: List[str] = []
    
    def parse_hash_fields(self, hash_str: str) -> Dict:
        """è§£æURLä¸­#åé¢çš„å­—ç¬¦ä¸²ï¼Œæå–è¦æ·»åŠ çš„å­—æ®µ"""
        fields = {}
        if not hash_str:
            return fields
            
        hash_str = hash_str.replace('"', '').strip()
        pattern = r'(\w+)\s*:\s*(\w+)'
        matches = re.findall(pattern, hash_str)
        
        for key, value in matches:
            fields[key.strip()] = value.strip()
            
        return fields
    
    def get_sites_from_urls(self, urls: List[str], target_names: List[str]) -> List[Dict]:
        """ä»å¤šä¸ªURLè¯»å–JSONæ•°æ®ï¼Œæ ¹æ®URLä¸­çš„å“ˆå¸Œå€¼æ·»åŠ å­—æ®µ"""
        self.results = []
        self.last_searched_names = target_names.copy()
        found_names = set()
        
        for url in urls:
            headers = {
                "User-Agent": "okhttp/5.0.0-alpha.14",
                "Accept": "application/json, text/javascript, */*; q=0.01"
            }
            
            try:
                if '#' in url:
                    base_url, hash_str = url.split('#', 1)
                    additional_fields = self.parse_hash_fields(hash_str)
                else:
                    base_url = url
                    additional_fields = {}
                
                print(f"æ­£åœ¨ä» {base_url} è¯»å–æ•°æ®...")
                response = requests.get(base_url, timeout=10, headers=headers)
                response.raise_for_status()
                data = response.json()
                
                if 'sites' in data and isinstance(data['sites'], list):
                    for site in data['sites']:
                        if 'name' in site and site['name'] in target_names:
                            if site['name'] not in found_names:
                                found_names.add(site['name'])
                                site_copy = site.copy()
                                if additional_fields:
                                    site_copy.update(additional_fields)
                                self.results.append(site_copy)
                
            except requests.exceptions.RequestException as e:
                print(f"ä» {url} è¯·æ±‚æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            except json.JSONDecodeError:
                print(f"æ— æ³•è§£æ {url} è¿”å›çš„JSONæ•°æ®")
            except Exception as e:
                print(f"å¤„ç† {url} æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        
        for name in target_names:
            if name not in found_names:
                print(f"æ‰€æœ‰URLä¸­å‡æœªæ‰¾åˆ°åç§°ä¸º'{name}'çš„ç«™ç‚¹")
        
        return self.results
    
    def get_results(self) -> List[Dict]:
        return self.results
    
    def save_results_to_file(self, file_path: str) -> bool:
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False)
            print(f"ç»“æœå·²æˆåŠŸä¿å­˜åˆ° {file_path}")
            return True
        except Exception as e:
            print(f"ä¿å­˜ç»“æœåˆ°æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False

def download_json_file(url, filename):
    """ä»æŒ‡å®šURLä¸‹è½½JSONæ–‡ä»¶å¹¶ä¿å­˜åˆ°æœ¬åœ°"""
    headers = {
        "User-Agent": "okhttp/5.0.0-alpha.14",
        "Accept": "application/json, text/javascript, */*; q=0.01"
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        with open(filename, "w", encoding="utf-8") as file:
            file.write(response.text)
        print(f"æ–‡ä»¶å·²æˆåŠŸä¸‹è½½å¹¶ä¿å­˜ä¸º {filename}")
    else:
        print(f"æ— æ³•ä¸‹è½½æ–‡ä»¶ï¼ŒçŠ¶æ€ç : {response.status_code}")

def read_json_file(filename):
    """è¯»å–æœ¬åœ°JSONæ–‡ä»¶å¹¶è¿”å›æ•°æ®"""
    with open(filename, "r", encoding="utf-8") as file:
        data = json.load(file)
    return data

def write_json_file(data, filename):
    """å°†æ•°æ®å†™å›åˆ°æœ¬åœ°JSONæ–‡ä»¶"""
    with open(filename, "w", encoding="utf-8") as file:
        json.dump(data, file, ensure_ascii=False, indent=4)

def replace_spider_key(data, new_value):
    """åœ¨å­—å…¸ä¸­æŸ¥æ‰¾é”®ä¸º'spider'å¹¶æ›¿æ¢å…¶å€¼"""
    if "spider" in data:
        data["spider"] = new_value

def replace_string_in_dict(d, replacements):
    """é€’å½’åœ°åœ¨å­—å…¸ä¸­æœç´¢å¹¶æ›¿æ¢å­—ç¬¦ä¸²"""
    for key, value in d.items():
        if isinstance(value, dict):
            replace_string_in_dict(value, replacements)
        elif isinstance(value, list):
            for i in range(len(value)):
                if isinstance(value[i], dict):
                    replace_string_in_dict(value[i], replacements)
                elif isinstance(value[i], str):
                    for old_str, new_str in replacements.items():
                        if old_str in value[i]:
                            value[i] = value[i].replace(old_str, new_str)
        elif isinstance(value, str):
            for old_str, new_str in replacements.items():
                if old_str in value:
                    d[key] = d[key].replace(old_str, new_str)

def find_spider_value(emby_spider):
    """åœ¨å­—å…¸ä¸­æŸ¥æ‰¾é”®ä¸º'spider'å¹¶è¿”å›å…¶å€¼"""
    return emby_spider.get("spider")

def insert_sites(data, sites_to_insert):
    """åœ¨ sites æ•°ç»„çš„ç¬¬äºŒä¸ªä½ç½®æ’å…¥æ–°çš„ç«™ç‚¹ä¿¡æ¯"""
    if "sites" in data and isinstance(data["sites"], list):
        data["sites"][1:1] = sites_to_insert
        print(f"å·²æ’å…¥ {len(sites_to_insert)} ä¸ªç«™ç‚¹åˆ°sitesæ•°ç»„çš„ç¬¬äºŒä¸ªä½ç½®")
    else:
        print("æœªæ‰¾åˆ°æœ‰æ•ˆçš„ 'sites' åˆ—è¡¨")

def main():
    # é…ç½®å‚æ•°
    pg_url = "https://www.252035.xyz/p/jsm.json"
    filename = "jsm.json"
    emby_url = "http://tvbox.xn--4kq62z5rby2qupq9ub.top/"
    emby_filename = "wex.json"
    new_spider_value = "https://www.252035.xyz/p/pg.jar"
    replacements = {
        "./lib/tokenm.json": "https://bp.banye.tech:7777/pg/lib/tokenm?token=qunyouyouqun",
        "./lib/": "https://www.252035.xyz/p/lib/"
    }
    
    # ç”¨äºè·å–ç«™ç‚¹çš„URLå’Œç›®æ ‡åç§°
    json_urls = [
        "https://bp.banye.tech:7777/sub/qunyouyouqun/pg",
        "http://tvbox.xn--4kq62z5rby2qupq9ub.top/#jar:spider_value"  # è¿™é‡Œçš„spider_valueæ˜¯å ä½ç¬¦
    ]
    site_names = ["ğŸ®é€šç”¨ç±»å‹â”ƒé…ç½®ä¸­å¿ƒğŸ®", "ğŸ€„ï¸embyâ”ƒ4KğŸ€„ï¸", "ğŸ ç»™åŠ›â”ƒç™¾åº¦ğŸ ", "AList",
                  "ç”µæŠ¥è±†ç“£","ç”µæŠ¥æœç´¢","ç”µæŠ¥ç½‘é¡µ","é±¼ä½¬ç›˜æœ",
                  "è´¦å·æ›´æ–°","TGè±†ç“£","TGé¢‘é“æœç´¢","TGç¾¤ç»„æœç´¢"]
    
    # ä¸‹è½½JSONæ–‡ä»¶
    download_json_file(pg_url, filename)
    download_json_file(emby_url, emby_filename)

    # è¯»å–å¹¶å¤„ç†åŸå§‹æ•°æ®
    original_data = read_json_file(filename)
    print("åŸå§‹æ•°æ®:")
    print(json.dumps(original_data, indent=4, ensure_ascii=False))

    # æ›¿æ¢spideré”®å€¼å’Œå­—ç¬¦ä¸²
    replace_spider_key(original_data, new_spider_value)
    replace_string_in_dict(original_data, replacements)

    # æå–embyçš„spiderå€¼ï¼ˆè¿™æ˜¯å®é™…éœ€è¦æ›¿æ¢çš„å€¼ï¼‰
    emby_data = read_json_file(emby_filename)
    actual_spider_value = find_spider_value(emby_data)  # è·å–wex.jsonä¸­çš„çœŸå®spiderå€¼
    print("\nä»wex.jsonä¸­è·å–çš„å®é™…spiderå€¼:")
    print(actual_spider_value)

    # ä»URLè·å–ç«™ç‚¹ä¿¡æ¯
    finder = SiteFinderWithHashFields()
    new_sites = finder.get_sites_from_urls(json_urls, site_names)
    
    # å…³é”®ä¿®å¤ï¼šå°†ç«™ç‚¹ä¸­jarå­—æ®µçš„"spider_value"æ›¿æ¢ä¸ºå®é™…çš„spiderå€¼
    for site in new_sites:
        if "jar" in site and site["jar"] == "spider_value":
            site["jar"] = actual_spider_value  # æ›¿æ¢å ä½ç¬¦ä¸ºå®é™…å€¼
    
    finder.save_results_to_file("banye.json")  # ä¿å­˜æ›¿æ¢åçš„ç«™ç‚¹
    
    # æ’å…¥è·å–åˆ°çš„ç«™ç‚¹åˆ°ç›®æ ‡JSON
    insert_sites(original_data, new_sites)

    # ä¿å­˜ä¿®æ”¹åçš„æ–‡ä»¶
    write_json_file(original_data, filename)
    print("\nä¿®æ”¹åçš„æ•°æ®:")
    print(json.dumps(original_data, indent=4, ensure_ascii=False))

if __name__ == "__main__":
    main()
